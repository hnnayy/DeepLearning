{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2SC/PjgB1D0XM5poL1wRb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hnnayy/DeepLearning/blob/main/week8-16/The%20Fundamentals%20of%20Machine%20Learning/Chapter_4-9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 4: Melatih Model (Training Models)\n",
        "\n",
        "Bab ini menggali lebih dalam mekanisme model Machine Learning, beralih dari sekadar menggunakannya sebagai \"kotak hitam\". Pemahaman ini krusial untuk memilih algoritma yang tepat, menyetel hyperparameter, melakukan analisis kesalahan, dan menjadi fondasi untuk jaringan saraf.\n",
        "\n",
        "### 1. Regresi Linear\n",
        "\n",
        "Regresi Linear adalah model fundamental yang memprediksi nilai target berdasarkan hubungan linear dengan fitur input.\n",
        "\n",
        "#### 1.1. Model Regresi Linear\n",
        "\n",
        "**Bentuk Umum:** Prediksi $\\hat{y}$ dari model Regresi Linear dengan $n$ fitur:\n",
        "$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n$$\n",
        "\n",
        "**Bentuk Vektorisasi:** Bentuk ringkas yang efisien untuk komputasi:\n",
        "$$\\hat{y} = h_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\boldsymbol{\\theta}^{\\text{T}}\\mathbf{x}$$\n",
        "\n",
        "Di mana:\n",
        "- $\\boldsymbol{\\theta}$ adalah **vektor parameter model**\n",
        "- $\\mathbf{x}$ adalah **vektor fitur instance** (dengan $x_0=1$ untuk bias term)\n",
        "\n",
        "#### 1.2. Fungsi Biaya MSE (Mean Squared Error)\n",
        "\n",
        "MSE mengukur tingkat kesalahan model sebagai rata-rata kuadrat perbedaan antara prediksi dan nilai target:\n",
        "\n",
        "$$\\text{MSE}(\\mathbf{X}, h_{\\boldsymbol{\\theta}}) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\boldsymbol{\\theta}^{\\text{T}}\\mathbf{x}^{(i)} - y^{(i)} \\right)^2$$\n",
        "\n",
        "**Tujuan:** Menemukan $\\boldsymbol{\\theta}$ yang meminimalkan MSE.\n",
        "\n",
        "#### 1.3. Persamaan Normal (Normal Equation)\n",
        "\n",
        "Solusi closed-form untuk menemukan $\\boldsymbol{\\theta}$ optimal:\n",
        "\n",
        "$$\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^{\\text{T}}\\mathbf{X})^{-1} \\mathbf{X}^{\\text{T}}\\mathbf{y}$$\n",
        "\n",
        "**Kompleksitas Komputasi:** $O(n^{2.4})$ hingga $O(n^3)$ tergantung implementasi.\n",
        "\n",
        "**Keunggulan:**\n",
        "- Tidak memerlukan feature scaling\n",
        "- Tidak memerlukan hyperparameter tuning\n",
        "- Solusi langsung tanpa iterasi\n",
        "\n",
        "**Kelemahan:**\n",
        "- Lambat untuk dataset besar ($n > 10,000$)\n",
        "- Memerlukan $\\mathbf{X}^{\\text{T}}\\mathbf{X}$ yang invertible\n",
        "\n",
        "### 2. Gradient Descent (Penurunan Gradien)\n",
        "\n",
        "Ketika Normal Equation tidak praktis, **Gradient Descent** menawarkan metode optimasi iteratif yang secara bertahap menyesuaikan parameter ke arah penurunan paling curam dari fungsi biaya.\n",
        "\n",
        "#### 2.1. Konsep Dasar\n",
        "\n",
        "GD dimulai dengan parameter acak $\\boldsymbol{\\theta}$, lalu berulang kali menghitung gradien dan memperbarui parameter:\n",
        "\n",
        "$$\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\eta \\nabla_{\\boldsymbol{\\theta}} \\text{MSE}(\\boldsymbol{\\theta}^{(t)})$$\n",
        "\n",
        "Di mana $\\eta$ adalah **learning rate** yang mengontrol ukuran langkah.\n",
        "\n",
        "**Gradien MSE untuk Linear Regression:**\n",
        "$$\\nabla_{\\boldsymbol{\\theta}} \\text{MSE}(\\boldsymbol{\\theta}) = \\frac{2}{m} \\mathbf{X}^{\\text{T}} (\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y})$$\n",
        "\n",
        "#### 2.2. Varian Gradient Descent\n",
        "\n",
        "| Varian | Batch Size | Kecepatan | Konvergensi | Use Case |\n",
        "|--------|------------|-----------|-------------|----------|\n",
        "| **Batch GD** | $m$ (full dataset) | Lambat | Stabil | Dataset kecil |\n",
        "| **Stochastic GD** | 1 | Cepat | Noisy | Online learning |\n",
        "| **Mini-batch GD** | $10 \\leq k \\leq 1000$ | Sedang | Balanced | Umum digunakan |\n",
        "\n",
        "**Learning Schedule:** Untuk SGD, learning rate sering dikurangi secara bertahap:\n",
        "$$\\eta(t) = \\frac{\\eta_0}{t + t_1}$$\n",
        "\n",
        "### 3. Regresi Polinomial\n",
        "\n",
        "Untuk dataset nonlinear, **Polynomial Regression** menambahkan pangkat fitur sebagai fitur baru:\n",
        "\n",
        "$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_1^3 + \\dots + \\theta_n x_1^n$$\n",
        "\n",
        "**Implementasi:** Menggunakan `PolynomialFeatures` transformer dari Scikit-Learn.\n",
        "\n",
        "#### 3.1. Learning Curves dan Bias-Variance Tradeoff\n",
        "\n",
        "**Learning Curves** memplot error training dan validation vs ukuran dataset:\n",
        "\n",
        "- **High Bias (Underfitting):**\n",
        "  - Training error tinggi\n",
        "  - Validation error tinggi\n",
        "  - Gap kecil antara keduanya\n",
        "  \n",
        "- **High Variance (Overfitting):**\n",
        "  - Training error rendah\n",
        "  - Validation error tinggi\n",
        "  - Gap besar antara keduanya\n",
        "\n",
        "### 4. Regularized Linear Models\n",
        "\n",
        "Untuk mengurangi overfitting, regularisasi menambahkan penalty term ke fungsi biaya.\n",
        "\n",
        "#### 4.1. Ridge Regression (L2 Regularization)\n",
        "\n",
        "$$J(\\boldsymbol{\\theta}) = \\text{MSE}(\\boldsymbol{\\theta}) + \\alpha \\sum_{i=1}^{n} \\theta_i^2$$\n",
        "\n",
        "**Closed-form solution:**\n",
        "$$\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^{\\text{T}}\\mathbf{X} + \\alpha \\mathbf{I})^{-1} \\mathbf{X}^{\\text{T}}\\mathbf{y}$$\n",
        "\n",
        "**Karakteristik:**\n",
        "- Menyusutkan koefisien mendekati nol\n",
        "- Tidak melakukan feature selection\n",
        "- Bekerja baik dengan multicollinearity\n",
        "\n",
        "#### 4.2. Lasso Regression (L1 Regularization)\n",
        "\n",
        "$$J(\\boldsymbol{\\theta}) = \\text{MSE}(\\boldsymbol{\\theta}) + \\alpha \\sum_{i=1}^{n} |\\theta_i|$$\n",
        "\n",
        "**Karakteristik:**\n",
        "- Dapat membuat koefisien menjadi tepat nol\n",
        "- Melakukan automatic feature selection\n",
        "- Tidak memiliki closed-form solution\n",
        "\n",
        "#### 4.3. Elastic Net\n",
        "\n",
        "Kombinasi Ridge dan Lasso:\n",
        "$$J(\\boldsymbol{\\theta}) = \\text{MSE}(\\boldsymbol{\\theta}) + r\\alpha \\sum_{i=1}^{n} |\\theta_i| + \\frac{1-r}{2}\\alpha \\sum_{i=1}^{n} \\theta_i^2$$\n",
        "\n",
        "**Parameter:**\n",
        "- $r \\in [0,1]$: mixing ratio\n",
        "- $r = 0$: Pure Ridge\n",
        "- $r = 1$: Pure Lasso\n",
        "\n",
        "#### 4.4. Early Stopping\n",
        "\n",
        "Teknik regularisasi yang menghentikan training ketika validation error mulai meningkat.\n",
        "\n",
        "### 5. Logistic Regression\n",
        "\n",
        "Model untuk klasifikasi yang memperkirakan probabilitas menggunakan logistic function.\n",
        "\n",
        "#### 5.1. Logistic Function (Sigmoid)\n",
        "\n",
        "$$\\sigma(t) = \\frac{1}{1 + e^{-t}}$$\n",
        "\n",
        "**Probabilitas prediksi:**\n",
        "$$\\hat{p} = h_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\sigma(\\boldsymbol{\\theta}^{\\text{T}}\\mathbf{x})$$\n",
        "\n",
        "#### 5.2. Cost Function (Log Loss)\n",
        "\n",
        "Untuk single training instance:\n",
        "$$c(\\boldsymbol{\\theta}) = \\begin{cases}\n",
        "-\\log(\\hat{p}) & \\text{if } y = 1 \\\\\n",
        "-\\log(1 - \\hat{p}) & \\text{if } y = 0\n",
        "\\end{cases}$$\n",
        "\n",
        "**Cost function untuk entire training set:**\n",
        "$$J(\\boldsymbol{\\theta}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{p}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{p}^{(i)}) \\right]$$\n",
        "\n",
        "#### 5.3. Softmax Regression (Multinomial Logistic Regression)\n",
        "\n",
        "Untuk klasifikasi multikelas dengan $K$ kelas:\n",
        "\n",
        "**Softmax function:**\n",
        "$$\\hat{p}_k = \\sigma(\\mathbf{s}(\\mathbf{x}))_k = \\frac{e^{s_k(\\mathbf{x})}}{\\sum_{j=1}^{K} e^{s_j(\\mathbf{x})}}$$\n",
        "\n",
        "**Cross-entropy cost function:**\n",
        "$$J(\\boldsymbol{\\Theta}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(\\hat{p}_k^{(i)})$$\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 5: Support Vector Machines (SVMs)\n",
        "\n",
        "SVM adalah algoritma powerful untuk klasifikasi linear dan nonlinear, regresi, dan outlier detection.\n",
        "\n",
        "### 1. Linear SVM Classification\n",
        "\n",
        "#### 1.1. Large Margin Classification\n",
        "\n",
        "SVM mencari decision boundary yang memaksimalkan **margin** - jarak antara decision boundary dan closest training instances dari kedua kelas.\n",
        "\n",
        "**Decision function:**\n",
        "$$h_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w}^{\\text{T}}\\mathbf{x} + b$$\n",
        "\n",
        "**Prediction:**\n",
        "$$\\hat{y} = \\begin{cases}\n",
        "0 & \\text{if } h_{\\mathbf{w},b}(\\mathbf{x}) < 0 \\\\\n",
        "1 & \\text{if } h_{\\mathbf{w},b}(\\mathbf{x}) \\geq 0\n",
        "\\end{cases}$$\n",
        "\n",
        "#### 1.2. Hard Margin Classification\n",
        "\n",
        "Untuk linearly separable data, SVM memecahkan:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\text{minimize} \\quad & \\frac{1}{2}\\mathbf{w}^{\\text{T}}\\mathbf{w} \\\\\n",
        "\\text{subject to} \\quad & t^{(i)}(\\mathbf{w}^{\\text{T}}\\mathbf{x}^{(i)} + b) \\geq 1 \\text{ for } i = 1, 2, \\ldots, m\n",
        "\\end{aligned}$$\n",
        "\n",
        "#### 1.3. Soft Margin Classification\n",
        "\n",
        "Untuk non-separable data atau outlier tolerance:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\text{minimize} \\quad & \\frac{1}{2}\\mathbf{w}^{\\text{T}}\\mathbf{w} + C \\sum_{i=1}^{m} \\zeta^{(i)} \\\\\n",
        "\\text{subject to} \\quad & t^{(i)}(\\mathbf{w}^{\\text{T}}\\mathbf{x}^{(i)} + b) \\geq 1 - \\zeta^{(i)} \\text{ and } \\zeta^{(i)} \\geq 0\n",
        "\\end{aligned}$$\n",
        "\n",
        "**Hyperparameter $C$:**\n",
        "- $C$ besar: Hard margin (low bias, high variance)\n",
        "- $C$ kecil: Soft margin (high bias, low variance)\n",
        "\n",
        "### 2. Nonlinear SVM Classification\n",
        "\n",
        "#### 2.1. Polynomial Kernel\n",
        "\n",
        "$$K(\\mathbf{a}, \\mathbf{b}) = (\\gamma \\mathbf{a}^{\\text{T}}\\mathbf{b} + r)^d$$\n",
        "\n",
        "**Parameters:**\n",
        "- $d$: degree of polynomial\n",
        "- $r$: coef0 (independent term)\n",
        "- $\\gamma$: kernel coefficient\n",
        "\n",
        "#### 2.2. Gaussian RBF Kernel\n",
        "\n",
        "$$K(\\mathbf{a}, \\mathbf{b}) = \\exp\\left(-\\gamma \\|\\mathbf{a} - \\mathbf{b}\\|^2\\right)$$\n",
        "\n",
        "**Parameter $\\gamma$:**\n",
        "- $\\gamma$ tinggi: Narrow bell-shaped curve (high variance, low bias)\n",
        "- $\\gamma$ rendah: Wide bell-shaped curve (low variance, high bias)\n",
        "\n",
        "#### 2.3. Sigmoid Kernel\n",
        "\n",
        "$$K(\\mathbf{a}, \\mathbf{b}) = \\tanh(\\gamma \\mathbf{a}^{\\text{T}}\\mathbf{b} + r)$$\n",
        "\n",
        "### 3. SVM Regression\n",
        "\n",
        "SVM regression mencoba fit sebanyak mungkin instances dalam \"street\" (margin) dengan lebar $2\\epsilon$.\n",
        "\n",
        "**Cost function:**\n",
        "$$J(\\mathbf{w}, b) = \\frac{1}{2}\\mathbf{w}^{\\text{T}}\\mathbf{w} + C \\sum_{i=1}^{m} \\zeta^{(i)}$$\n",
        "\n",
        "**Constraints:**\n",
        "$$|t^{(i)} - \\mathbf{w}^{\\text{T}}\\mathbf{x}^{(i)} - b| \\leq \\epsilon + \\zeta^{(i)}$$\n",
        "\n",
        "### 4. Computational Complexity\n",
        "\n",
        "| Algorithm | Training | Prediction |\n",
        "|-----------|----------|------------|\n",
        "| LinearSVC | $O(m \\times n)$ | $O(n)$ |\n",
        "| SVC | $O(m^2 \\times n)$ to $O(m^3 \\times n)$ | $O(n_s \\times n)$ |\n",
        "\n",
        "Di mana $n_s$ adalah jumlah support vectors.\n",
        "\n",
        "---\n",
        "\n",
        "## Bab 6: Decision Trees\n",
        "\n",
        "Decision Trees adalah model versatile yang dapat digunakan untuk klasifikasi dan regresi, mudah diinterpretasi dan tidak memerlukan feature scaling.\n",
        "\n",
        "### 1. Making Predictions\n",
        "\n",
        "Decision Tree membuat prediksi dengan:\n",
        "1. Dimulai dari root node\n",
        "2. Test feature value di setiap internal node  \n",
        "3. Ikuti branch berdasarkan hasil test\n",
        "4. Sampai mencapai leaf node yang memberikan prediksi\n",
        "\n",
        "### 2. Estimating Class Probabilities\n",
        "\n",
        "Untuk klasifikasi, Decision Tree memperkirakan probabilitas kelas berdasarkan rasio training instances dari setiap kelas di leaf node:\n",
        "\n",
        "$$\\hat{p}_{k,\\text{node}} = \\frac{n_{k,\\text{node}}}{n_{\\text{node}}}$$\n",
        "\n",
        "### 3. CART Training Algorithm\n",
        "\n",
        "Scikit-Learn menggunakan **Classification and Regression Tree (CART)** algorithm:\n",
        "\n",
        "#### 3.1. Cost Function untuk Klasifikasi\n",
        "\n",
        "**Gini Impurity:**\n",
        "$$G_i = 1 - \\sum_{k=1}^{n} p_{i,k}^2$$\n",
        "\n",
        "**Entropy:**\n",
        "$$H_i = -\\sum_{k=1}^{n} p_{i,k} \\log_2(p_{i,k})$$\n",
        "\n",
        "**CART Cost Function:**\n",
        "$$J(k, t_k) = \\frac{m_{\\text{left}}}{m} G_{\\text{left}} + \\frac{m_{\\text{right}}}{m} G_{\\text{right}}$$\n",
        "\n",
        "#### 3.2. Cost Function untuk Regresi\n",
        "\n",
        "CART meminimalkan MSE:\n",
        "$$J(k, t_k) = \\frac{m_{\\text{left}}}{m} \\text{MSE}_{\\text{left}} + \\frac{m_{\\text{right}}}{m} \\text{MSE}_{\\text{right}}$$\n",
        "\n",
        "Di mana:\n",
        "$$\\text{MSE}_{\\text{node}} = \\sum_{i \\in \\text{node}} (\\hat{y}_{\\text{node}} - y^{(i)})^2$$\n",
        "$$\\hat{y}_{\\text{node}} = \\frac{1}{m_{\\text{node}}} \\sum_{i \\in \\text{node}} y^{(i)}$$\n",
        "\n",
        "### 4. Regularization Hyperparameters\n",
        "\n",
        "Decision Trees prone terhadap overfitting. Hyperparameter untuk regularisasi:\n",
        "\n",
        "| Parameter | Deskripsi | Effect |\n",
        "|-----------|-----------|--------|\n",
        "| `max_depth` | Maximum depth | Mengurangi overfitting |\n",
        "| `min_samples_split` | Min samples to split node | Mencegah splits pada node kecil |\n",
        "| `min_samples_leaf` | Min samples in leaf | Smooth decision boundary |\n",
        "| `max_leaf_nodes` | Maximum leaf nodes | Alternative ke max_depth |\n",
        "| `max_features` | Max features per split | Menambah randomness |\n",
        "\n",
        "### 5. Kelemahan Decision Trees\n",
        "\n",
        "1. **High Variance:** Sensitif terhadap small changes dalam training data\n",
        "2. **Orthogonal Decision Boundaries:** Kesulitan dengan diagonal decision boundaries  \n",
        "3. **Overfitting:** Tanpa regularization, cenderung overfit\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 7: Ensemble Learning dan Random Forests\n",
        "\n",
        "Ensemble methods menggabungkan prediksi dari multiple models untuk menghasilkan prediksi yang lebih akurat dan robust.\n",
        "\n",
        "### 1. Voting Classifiers\n",
        "\n",
        "#### 1.1. Hard Voting\n",
        "\n",
        "Prediksi berdasarkan majority vote:\n",
        "$$\\hat{y} = \\text{mode}\\{h_1(\\mathbf{x}), h_2(\\mathbf{x}), \\ldots, h_N(\\mathbf{x})\\}$$\n",
        "\n",
        "#### 1.2. Soft Voting\n",
        "\n",
        "Prediksi berdasarkan rata-rata probabilitas:\n",
        "$$\\hat{y} = \\arg\\max_k \\frac{1}{N} \\sum_{i=1}^{N} \\hat{p}_{i,k}$$\n",
        "\n",
        "**Soft voting umumnya perform lebih baik** jika classifiers dapat estimate probabilities.\n",
        "\n",
        "### 2. Bagging dan Pasting\n",
        "\n",
        "Kedua metode menggunakan algoritma yang sama dengan subset training data yang berbeda.\n",
        "\n",
        "#### 2.1. Bootstrap Aggregating (Bagging)\n",
        "\n",
        "- **Sampling:** With replacement\n",
        "- **Bias:** Sedikit meningkat\n",
        "- **Variance:** Menurun signifikan\n",
        "- **Out-of-bag Evaluation:** Instance yang tidak pernah disampling untuk training\n",
        "\n",
        "**Out-of-bag Score:**\n",
        "$$\\text{oob score} = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{I}[\\hat{y}_{\\text{oob}}^{(i)} = y^{(i)}]$$\n",
        "\n",
        "#### 2.2. Pasting\n",
        "\n",
        "- **Sampling:** Without replacement\n",
        "- **Parallelization:** Perfect untuk parallel training\n",
        "\n",
        "### 3. Random Forests\n",
        "\n",
        "Random Forest = Bagging + Random Feature Selection\n",
        "\n",
        "#### 3.1. Extra Randomness\n",
        "\n",
        "Pada setiap node split:\n",
        "1. Random subset dari features dipilih\n",
        "2. Best threshold dicari hanya dari subset ini\n",
        "\n",
        "**Typical setting:** $\\text{max\\_features} = \\sqrt{n}$ untuk klasifikasi, $n/3$ untuk regresi.\n",
        "\n",
        "#### 3.2. Feature Importance\n",
        "\n",
        "Random Forest menghitung feature importance berdasarkan weighted average depth:\n",
        "\n",
        "$$\\text{importance}(f) = \\frac{\\sum_{\\text{nodes using } f} w_{\\text{node}} \\times \\text{impurity decrease}}{\\sum_{\\text{all nodes}} w_{\\text{node}} \\times \\text{impurity decrease}}$$\n",
        "\n",
        "### 4. Boosting\n",
        "\n",
        "Boosting trains predictors sequentially, dengan setiap predictor mencoba memperbaiki predecessor-nya.\n",
        "\n",
        "#### 4.1. AdaBoost (Adaptive Boosting)\n",
        "\n",
        "**Algorithm:**\n",
        "1. Initialize uniform weights: $w^{(i)} = \\frac{1}{m}$\n",
        "2. For each predictor $j = 1, 2, \\ldots, N$:\n",
        "   - Train predictor $h_j$ dengan weighted training set\n",
        "   - Compute weighted error rate: $r_j = \\frac{\\sum_{i=1}^{m} w^{(i)} \\mathbb{I}[\\hat{y}_j^{(i)} \\neq y^{(i)}]}{\\sum_{i=1}^{m} w^{(i)}}$\n",
        "   - Compute predictor weight: $\\alpha_j = \\eta \\log\\frac{1-r_j}{r_j}$\n",
        "   - Update instance weights\n",
        "\n",
        "**Final prediction:**\n",
        "$$h(\\mathbf{x}) = \\text{sign}\\left(\\sum_{j=1}^{N} \\alpha_j h_j(\\mathbf{x})\\right)$$\n",
        "\n",
        "#### 4.2. Gradient Boosting\n",
        "\n",
        "**Algorithm:**\n",
        "1. Initialize dengan constant predictor: $F_0(\\mathbf{x}) = \\arg\\min_\\gamma \\sum_{i=1}^{m} L(y^{(i)}, \\gamma)$\n",
        "2. For $j = 1, 2, \\ldots, N$:\n",
        "   - Compute residuals: $r_{i,j} = -\\frac{\\partial L(y^{(i)}, F_{j-1}(\\mathbf{x}^{(i)}))}{\\partial F_{j-1}(\\mathbf{x}^{(i)})}$\n",
        "   - Fit regressor $h_j$ ke residuals\n",
        "   - Find optimal step size: $\\gamma_j = \\arg\\min_\\gamma \\sum_{i=1}^{m} L(y^{(i)}, F_{j-1}(\\mathbf{x}^{(i)}) + \\gamma h_j(\\mathbf{x}^{(i)}))$\n",
        "   - Update: $F_j(\\mathbf{x}) = F_{j-1}(\\mathbf{x}) + \\gamma_j h_j(\\mathbf{x})$\n",
        "\n",
        "### 5. Stacking (Stacked Generalization)\n",
        "\n",
        "**Two-level architecture:**\n",
        "1. **Level-0 models:** Base predictors\n",
        "2. **Level-1 model:** Meta-learner yang belajar cara combine predictions\n",
        "\n",
        "**Training Process:**\n",
        "1. Split training set menjadi dua subset\n",
        "2. Train level-0 models pada subset pertama\n",
        "3. Generate predictions untuk subset kedua\n",
        "4. Train meta-learner menggunakan predictions sebagai features\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 8: Dimensionality Reduction\n",
        "\n",
        "Dimensionality reduction mengurangi jumlah features sambil mempertahankan informasi penting.\n",
        "\n",
        "### 1. Curse of Dimensionality\n",
        "\n",
        "Dalam high-dimensional spaces:\n",
        "- **Sparsity:** Training instances tersebar sangat jarang\n",
        "- **Distance:** Semua distances menjadi hampir sama\n",
        "- **Overfitting:** Model complexity meningkat exponentially\n",
        "\n",
        "**Volume of unit hypersphere:**\n",
        "$$V_d = \\frac{\\pi^{d/2}}{\\Gamma(d/2 + 1)}$$\n",
        "\n",
        "### 2. Approaches to Dimensionality Reduction\n",
        "\n",
        "#### 2.1. Projection\n",
        "\n",
        "Memproyeksikan data ke subspace dengan dimensi lebih rendah.\n",
        "\n",
        "#### 2.2. Manifold Learning\n",
        "\n",
        "Banyak high-dimensional datasets sebenarnya terletak pada lower-dimensional manifold.\n",
        "\n",
        "**Manifold Hypothesis:** Real-world high-dimensional data cenderung terletak dekat dengan much lower-dimensional manifold.\n",
        "\n",
        "### 3. Principal Component Analysis (PCA)\n",
        "\n",
        "PCA mencari directions dengan maximum variance dan memproyeksikan data ke directions tersebut.\n",
        "\n",
        "#### 3.1. Mathematical Foundation\n",
        "\n",
        "**Covariance Matrix:**\n",
        "$$\\mathbf{C} = \\frac{1}{m-1} \\mathbf{X}^{\\text{T}}\\mathbf{X}$$\n",
        "\n",
        "**Principal Components:** Eigenvectors dari covariance matrix.\n",
        "\n",
        "**Singular Value Decomposition (SVD):**\n",
        "$$\\mathbf{X} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\text{T}}$$\n",
        "\n",
        "Principal components adalah columns dari $\\mathbf{V}$.\n",
        "\n",
        "#### 3.2. Projection ke d-Dimensional Subspace\n",
        "\n",
        "$$\\mathbf{X}_{d\\text{-proj}} = \\mathbf{X}\\mathbf{W}_d$$\n",
        "\n",
        "Di mana $\\mathbf{W}_d$ berisi first $d$ principal components.\n",
        "\n",
        "#### 3.3. Explained Variance Ratio\n",
        "\n",
        "Rasio varians yang dijelaskan oleh setiap principal component:\n",
        "\n",
        "$$\\text{explained variance ratio}_k = \\frac{\\sigma_k^2}{\\sum_{i=1}^{n} \\sigma_i^2}$$\n",
        "\n",
        "#### 3.4. Choosing Number of Dimensions\n",
        "\n",
        "Pilih $d$ such that:\n",
        "$$\\frac{\\sum_{i=1}^{d} \\sigma_i^2}{\\sum_{i=1}^{n} \\sigma_i^2} \\geq 0.95$$\n",
        "\n",
        "#### 3.5. PCA for Compression\n",
        "\n",
        "**Reconstruction:**\n",
        "$$\\mathbf{X}_{\\text{recovered}} = \\mathbf{X}_{d\\text{-proj}} \\mathbf{W}_d^{\\text{T}}$$\n",
        "\n",
        "**Reconstruction Error:**\n",
        "$$\\text{MSE} = \\frac{1}{m} \\|\\mathbf{X} - \\mathbf{X}_{\\text{recovered}}\\|_F^2$$\n",
        "\n",
        "### 4. Incremental PCA\n",
        "\n",
        "Untuk datasets yang tidak fit dalam memory:\n",
        "\n",
        "**Mini-batch PCA:**\n",
        "$$\\mathbf{C}_{\\text{new}} = \\frac{n_{\\text{old}}}{n_{\\text{old}} + n_{\\text{batch}}} \\mathbf{C}_{\\text{old}} + \\frac{n_{\\text{batch}}}{n_{\\text{old}} + n_{\\text{batch}}} \\mathbf{C}_{\\text{batch}}$$\n",
        "\n",
        "### 5. Kernel PCA\n",
        "\n",
        "PCA dalam feature space yang ditransformasi implicitly melalui kernel trick.\n",
        "\n",
        "**Kernel Matrix:**\n",
        "$$\\mathbf{K}_{i,j} = \\phi(\\mathbf{x}^{(i)})^{\\text{T}} \\phi(\\mathbf{x}^{(j)}) = K(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)})$$\n",
        "\n",
        "**Popular Kernels:**\n",
        "- **Linear:** $K(\\mathbf{a}, \\mathbf{b}) = \\mathbf{a}^{\\text{T}}\\mathbf{b}$\n",
        "- **RBF:** $K(\\mathbf{a}, \\mathbf{b}) = \\exp(-\\gamma \\|\\mathbf{a} - \\mathbf{b}\\|^2)$\n",
        "- **Sigmoid:** $K(\\mathbf{a}, \\mathbf{b}) = \\tanh(\\gamma \\mathbf{a}^{\\text{T}}\\mathbf{b} + r)$\n",
        "\n",
        "### 6. Other Dimensionality Reduction Techniques\n",
        "\n",
        "#### 6.1. Locally Linear Embedding (LLE)\n",
        "\n",
        "LLE preserves local relationships:\n",
        "\n",
        "1. **Find k-nearest neighbors** untuk setiap instance\n",
        "2. **Compute weights** yang reconstruct setiap instance sebagai linear combination dari neighbors\n",
        "3. **Map ke lower dimension** sambil preserving weights\n",
        "\n",
        "#### 6.2. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "t-SNE sangat baik untuk visualization:\n",
        "\n",
        "**Similarity dalam original space:**\n",
        "$$p_{j|i} = \\frac{\\exp(-\\|\\mathbf{x}^{(i)} - \\mathbf{x}^{(j)}\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|\\mathbf{x}^{(i)} - \\mathbf{x}^{(k)}\\|^2 / 2\\sigma_i^2)}$$\n",
        "\n",
        "**Similarity dalam embedded space:**\n",
        "$$q_{ij} = \\frac{(1 + \\|\\mathbf{y}^{(i)} - \\mathbf{y}^{(j)}\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|\\mathbf{y}^{(k)} - \\mathbf{y}^{(l)}\\|^2)^{-1}}$$\n",
        "\n",
        "**Cost Function (KL Divergence):**\n",
        "$$C = \\sum_i \\text{KL}(P_i \\| Q_i) = \\sum_i \\sum_j p_{j|i} \\log \\frac{p_{j|i}}{q_{ij}}$$\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 9: Unsupervised Learning\n",
        "\n",
        "Unsupervised learning bekerja dengan data tanpa labels, mencari hidden patterns dan structures.\n",
        "\n",
        "### 1. Clustering\n",
        "\n",
        "#### 1.1. K-Means\n",
        "\n",
        "**Algorithm:**\n",
        "1. Initialize $k$ centroids randomly\n",
        "2. Repeat until convergence:\n",
        "   - **Assignment step:** Assign setiap instance ke closest centroid\n",
        "     $c^{(i)} = \\arg\\min_j \\|\\mathbf{x}^{(i)} - \\boldsymbol{\\mu}^{(j)}\\|^2$\n",
        "   - **Update step:** Update centroids\n",
        "     $\\boldsymbol{\\mu}^{(j)} = \\frac{1}{|S_j|} \\sum_{\\mathbf{x} \\in S_j} \\mathbf{x}$\n",
        "\n",
        "**Cost Function (Inertia):**\n",
        "$J = \\sum_{i=1}^{m} \\|\\mathbf{x}^{(i)} - \\boldsymbol{\\mu}^{(c^{(i)})}\\|^2$\n",
        "\n",
        "**Computational Complexity:** $O(m \\times k \\times n \\times i)$\n",
        "- $m$: number of instances\n",
        "- $k$: number of clusters  \n",
        "- $n$: number of features\n",
        "- $i$: number of iterations\n",
        "\n",
        "#### 1.2. K-Means++\n",
        "\n",
        "Algoritma initialization yang lebih baik:\n",
        "\n",
        "1. Choose first centroid randomly\n",
        "2. For each remaining centroid:\n",
        "   - Choose next centroid dengan probability proportional to squared distance dari nearest existing centroid\n",
        "   - $P(\\mathbf{x}^{(i)}) = \\frac{D(\\mathbf{x}^{(i)})^2}{\\sum_j D(\\mathbf{x}^{(j)})^2}$\n",
        "\n",
        "#### 1.3. Mini-batch K-Means\n",
        "\n",
        "Untuk large datasets:\n",
        "- Use random sample (mini-batch) di setiap iteration\n",
        "- Update centroids menggunakan gradient descent\n",
        "- Much faster dengan sedikit degradation dalam quality\n",
        "\n",
        "#### 1.4. Finding Optimal K\n",
        "\n",
        "**Elbow Method:**\n",
        "Plot inertia vs $k$ dan cari \"elbow point\"\n",
        "\n",
        "**Silhouette Analysis:**\n",
        "$s^{(i)} = \\frac{b^{(i)} - a^{(i)}}{\\max(a^{(i)}, b^{(i)})}$\n",
        "\n",
        "Di mana:\n",
        "- $a^{(i)}$: mean distance ke instances dalam cluster yang sama\n",
        "- $b^{(i)}$: mean distance ke instances dalam nearest cluster\n",
        "\n",
        "**Silhouette Score:** $s \\in [-1, +1]$\n",
        "- $s \\approx +1$: Instance well inside its cluster\n",
        "- $s \\approx 0$: Instance close to cluster boundary  \n",
        "- $s \\approx -1$: Instance might be in wrong cluster\n",
        "\n",
        "### 2. DBSCAN (Density-Based Spatial Clustering)\n",
        "\n",
        "DBSCAN dapat find clusters dengan arbitrary shapes dan identify outliers.\n",
        "\n",
        "#### 2.1. Core Concepts\n",
        "\n",
        "**$\\epsilon$-neighborhood:** $N_\\epsilon(\\mathbf{x}) = \\{\\mathbf{y} \\in D | \\text{dist}(\\mathbf{x}, \\mathbf{y}) \\leq \\epsilon\\}$\n",
        "\n",
        "**Core Point:** Point dengan at least `MinPts` points dalam $\\epsilon$-neighborhood\n",
        "\n",
        "**Border Point:** Non-core point dalam $\\epsilon$-neighborhood dari core point\n",
        "\n",
        "**Noise Point:** Neither core nor border point\n",
        "\n",
        "#### 2.2. Algorithm\n",
        "\n",
        "1. **For each unvisited point $\\mathbf{p}$:**\n",
        "   - Mark $\\mathbf{p}$ sebagai visited\n",
        "   - If $\\mathbf{p}$ adalah core point:\n",
        "     - Create new cluster dengan $\\mathbf{p}$\n",
        "     - Add all points dalam $N_\\epsilon(\\mathbf{p})$ ke cluster\n",
        "     - For each point $\\mathbf{q}$ dalam $N_\\epsilon(\\mathbf{p})$:\n",
        "       - If $\\mathbf{q}$ unvisited: mark sebagai visited, dan if core point, add $N_\\epsilon(\\mathbf{q})$ ke cluster\n",
        "   - Else: mark $\\mathbf{p}$ sebagai noise\n",
        "\n",
        "#### 2.3. Hyperparameters\n",
        "\n",
        "- **$\\epsilon$ (eps):** Maximum distance antara two points untuk dianggap neighbors\n",
        "- **MinPts:** Minimum number of points untuk form dense region\n",
        "\n",
        "**Rule of thumb:** MinPts $\\geq$ dimensionality + 1\n",
        "\n",
        "### 3. Hierarchical Clustering\n",
        "\n",
        "#### 3.1. Agglomerative Clustering (Bottom-up)\n",
        "\n",
        "**Algorithm:**\n",
        "1. Start dengan each instance sebagai separate cluster\n",
        "2. Repeatedly merge two closest clusters\n",
        "3. Stop ketika hanya satu cluster tersisa\n",
        "\n",
        "**Linkage Criteria:**\n",
        "- **Single linkage:** $d_{\\min}(A,B) = \\min_{\\mathbf{a} \\in A, \\mathbf{b} \\in B} \\|\\mathbf{a} - \\mathbf{b}\\|$\n",
        "- **Complete linkage:** $d_{\\max}(A,B) = \\max_{\\mathbf{a} \\in A, \\mathbf{b} \\in B} \\|\\mathbf{a} - \\mathbf{b}\\|$\n",
        "- **Average linkage:** $d_{\\text{avg}}(A,B) = \\frac{1}{|A||B|} \\sum_{\\mathbf{a} \\in A} \\sum_{\\mathbf{b} \\in B} \\|\\mathbf{a} - \\mathbf{b}\\|$\n",
        "- **Ward linkage:** Minimizes within-cluster sum of squares\n",
        "\n",
        "#### 3.2. Dendrogram\n",
        "\n",
        "Tree diagram yang shows hierarchical relationship antara clusters.\n",
        "\n",
        "**Cophenetic Distance:** Distance dalam dendrogram antara two points.\n",
        "\n",
        "### 4. Gaussian Mixture Models (GMM)\n",
        "\n",
        "GMM assumes data comes dari mixture of Gaussian distributions.\n",
        "\n",
        "#### 4.1. Mathematical Model\n",
        "\n",
        "**Probability density:**\n",
        "$p(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$\n",
        "\n",
        "Di mana:\n",
        "- $\\pi_k$: mixing coefficient ($\\sum_k \\pi_k = 1$)\n",
        "- $\\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$: multivariate Gaussian\n",
        "\n",
        "**Multivariate Gaussian:**\n",
        "$\\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu})^{\\text{T}}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})\\right)$\n",
        "\n",
        "#### 4.2. Expectation-Maximization (EM) Algorithm\n",
        "\n",
        "**E-step (Expectation):**\n",
        "Compute posterior probabilities:\n",
        "$\\gamma_{ik} = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}^{(i)} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(\\mathbf{x}^{(i)} | \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}$\n",
        "\n",
        "**M-step (Maximization):**\n",
        "Update parameters:\n",
        "$N_k = \\sum_{i=1}^{m} \\gamma_{ik}$\n",
        "$\\boldsymbol{\\mu}_k = \\frac{1}{N_k} \\sum_{i=1}^{m} \\gamma_{ik} \\mathbf{x}^{(i)}$\n",
        "$\\boldsymbol{\\Sigma}_k = \\frac{1}{N_k} \\sum_{i=1}^{m} \\gamma_{ik} (\\mathbf{x}^{(i)} - \\boldsymbol{\\mu}_k)(\\mathbf{x}^{(i)} - \\boldsymbol{\\mu}_k)^{\\text{T}}$\n",
        "$\\pi_k = \\frac{N_k}{m}$\n",
        "\n",
        "### 5. Anomaly Detection\n",
        "\n",
        "#### 5.1. Gaussian-based Anomaly Detection\n",
        "\n",
        "**Assumption:** Normal data follows multivariate Gaussian distribution.\n",
        "\n",
        "**Anomaly score:**\n",
        "$p(\\mathbf{x}) = \\prod_{j=1}^{n} p(x_j; \\mu_j, \\sigma_j^2)$\n",
        "\n",
        "**Decision rule:**\n",
        "- If $p(\\mathbf{x}) < \\epsilon$: anomaly\n",
        "- If $p(\\mathbf{x}) \\geq \\epsilon$: normal\n",
        "\n",
        "#### 5.2. One-Class SVM\n",
        "\n",
        "Learns decision boundary yang tightly encloses normal data.\n",
        "\n",
        "**Objective:**\n",
        "$\\min_{\\mathbf{w}, \\xi, \\rho} \\frac{1}{2}\\|\\mathbf{w}\\|^2 + \\frac{1}{\\nu m} \\sum_{i=1}^{m} \\xi_i - \\rho$\n",
        "\n",
        "**Subject to:**\n",
        "$\\mathbf{w}^{\\text{T}}\\phi(\\mathbf{x}^{(i)}) \\geq \\rho - \\xi_i, \\quad \\xi_i \\geq 0$\n",
        "\n",
        "#### 5.3. Isolation Forest\n",
        "\n",
        "**Key Insight:** Anomalies are easier to isolate (require fewer splits).\n",
        "\n",
        "**Algorithm:**\n",
        "1. Build ensemble of isolation trees (iTrees)\n",
        "2. For each iTree:\n",
        "   - Randomly select feature dan split value\n",
        "   - Recursively partition data until isolated\n",
        "3. **Anomaly Score:**\n",
        "   $s(\\mathbf{x}, n) = 2^{-\\frac{E(h(\\mathbf{x}))}{c(n)}}$\n",
        "   \n",
        "   Di mana:\n",
        "   - $E(h(\\mathbf{x}))$: average path length\n",
        "   - $c(n) = 2H(n-1) - \\frac{2(n-1)}{n}$: average path length dari unsuccessful search dalam BST\n",
        "\n",
        "### 6. Novelty Detection\n",
        "\n",
        "Mirip dengan anomaly detection, tetapi training set diasumsikan \"clean\" (tidak ada outliers).\n",
        "\n",
        "**Difference:**\n",
        "- **Anomaly Detection:** Training set mungkin berisi outliers\n",
        "- **Novelty Detection:** Training set clean, detect novelties dalam test set\n",
        "\n",
        "### 7. Association Rule Learning\n",
        "\n",
        "Mencari frequent patterns dalam transactional data.\n",
        "\n",
        "#### 7.1. Market Basket Analysis\n",
        "\n",
        "**Support:** Frequency of itemset\n",
        "$\\text{support}(A) = \\frac{\\text{transactions containing } A}{\\text{total transactions}}$\n",
        "\n",
        "**Confidence:** Conditional probability\n",
        "$\\text{confidence}(A \\Rightarrow B) = \\frac{\\text{support}(A \\cup B)}{\\text{support}(A)}$\n",
        "\n",
        "**Lift:** How much more likely B is purchased when A is purchased\n",
        "$\\text{lift}(A \\Rightarrow B) = \\frac{\\text{confidence}(A \\Rightarrow B)}{\\text{support}(B)}$\n",
        "\n",
        "#### 7.2. Apriori Algorithm\n",
        "\n",
        "**Apriori Principle:** If itemset is infrequent, semua supersets-nya juga infrequent.\n",
        "\n",
        "**Algorithm:**\n",
        "1. Find all frequent 1-itemsets\n",
        "2. Use frequent $k$-itemsets untuk generate candidate $(k+1)$-itemsets\n",
        "3. Prune candidates yang violate Apriori principle\n",
        "4. Count support untuk remaining candidates\n",
        "5. Repeat until no more frequent itemsets found\n",
        "\n",
        "---\n",
        "\n",
        "## Summary dan Best Practices\n",
        "\n",
        "### Model Selection Guidelines\n",
        "\n",
        "| Problem Type | Recommended Algorithms | Considerations |\n",
        "|--------------|----------------------|----------------|\n",
        "| **Linear Problems** | Linear/Logistic Regression, SVM Linear | Fast, interpretable |\n",
        "| **Small Dataset** | SVM dengan RBF kernel | Good generalization |\n",
        "| **Large Dataset** | SGD, Random Forest, Gradient Boosting | Scalable algorithms |\n",
        "| **Need Interpretability** | Decision Trees, Linear Models | Explainable predictions |\n",
        "| **High Dimensional** | PCA + Linear Models | Dimensionality reduction |\n",
        "| **Mixed Data Types** | Random Forest, Gradient Boosting | Handle categorical features |\n",
        "\n",
        "### Hyperparameter Tuning Strategy\n",
        "\n",
        "1. **Start simple:** Begin dengan default parameters\n",
        "2. **Use validation:** Always use cross-validation\n",
        "3. **Grid search:** For small parameter spaces\n",
        "4. **Random search:** For large parameter spaces  \n",
        "5. **Bayesian optimization:** For expensive evaluations\n",
        "\n",
        "### Cross-Validation Best Practices\n",
        "\n",
        "```python\n",
        "# Stratified K-Fold untuk classification\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Time Series Split untuk temporal data\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "```\n",
        "\n",
        "### Feature Engineering Checklist\n",
        "\n",
        "- [ ] **Scaling:** StandardScaler, MinMaxScaler, RobustScaler\n",
        "- [ ] **Encoding:** OneHotEncoder, LabelEncoder, TargetEncoder\n",
        "- [ ] **Missing Values:** SimpleImputer, KNNImputer\n",
        "- [ ] **Feature Selection:** SelectKBest, RFE, SelectFromModel\n",
        "- [ ] **Dimensionality Reduction:** PCA, t-SNE untuk visualization\n",
        "\n",
        "---\n",
        "\n",
        "Rangkuman ini memberikan foundation yang solid untuk memahami berbagai algoritma Machine Learning, dari supervised hingga unsupervised learning, dengan mathematical foundations dan practical considerations untuk implementation yang sukses."
      ],
      "metadata": {
        "id": "gPu6Wu5ZB85Y"
      }
    }
  ]
}